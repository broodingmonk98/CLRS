In this chapter we study the divide and conquer paradigm, which essentially divides the problem, conquers the subproblems recursively, solving it directly if they are small enough, and then combine the solutions of the subproblems. The former is the recursive case, and solving the small problems directly is the base case. Recurrences are a natural way to specify running times of divide and conquer algorithms. They are equations or inequalities expressing a function in terms of its value on smaller inputs. General methods used for solving such recurrences are substitution method, recursion tree method and master method.

4.1
This section is devoted to dealing with the maximum subarray problem. It is stateted as our wanting to buy stock low and sell it high, thereby maximizing our profit. We can only buy and sell the stock once, and we are given the price of stock over the period of a few days. We have to determine when to buy and when to sell the stock. If we replace our input, with change in stock price from day to day, our problem becomes one of finding the maximum subarray in the given array.
	The naive way would be to take all possible buying and selling points (nC_2) and check our profit. This will take theta(n^2) time. A better way would be, using divide and conquer. The maximum subarray will either lie entirely in the left or entirely in the right half of your initial array, or it shall pass through the mid point. The first two cases are the recursive cases. The final case, takes theta(n) time to solve. To find max subarray passing through the midpoint, we start off at the middle and move to the left and right (all the way till the end) and remember at which left and right indices we got the highest value.
	Finally we combine the three answers (the max subarray in left, right and passing through midpoint), which is done by simple taking the max of these three values. The resultant recurrence is T(n) = 2T(n/2) + theta(n) and has a bound of theta(n log n). 
(Check the maxSubarray.c program in the programs folder)

4.2
This section deals with a divide and conquer method for matrix multiplication which beats the naive approach asymptotically. The naive approach has an theta(n^3) running time whereas the divide and conquer approach has a running time of theta(n^lg 7) running time. For the code for the naive matrix multiplication, check the naiveMatrixMul.c file in the programs directory.
The simple divide and conquer approach would be to break down the given matrices into four submatrices and multiply and add them separately(see divideMatrixMul.c in programs directory). However this gives the same running time of theta(n^3) and we are no better off. This has the recurrence T(n) = 8T(n/2) + theta(n^2). Strassen's algorithm for matrix multiplication brings down the recurrence to T(n) = 7T(n/2) + theta(n^2) and although the constant terms hidden in the asymptotic notation are quite large, it beats the naive approach asymptotically. The book says that this method is very unintutive and so I won't be going into how it works here. You can check the strassens.c file in subdirectory for the code.

4.3
This section tells us how to prove that something is a valid solution to a recurrence. How we obtain that something is through guessing, atleast for now, but once we have a guess we can either prove or disprove it using the substitution method for solving recurrences.
We use basic induction, and if we have a recurrence relation, and guess that its solution is O(f(x)), we assume that its true for lower values below an n, and then show that it holds for T(n) by substituting it in the recurrence (hence the name). We then need to show the base cases, so for say T(1). However if T(1) proves problamatic we can show it for any fixed T(c). Sometimes having a weak upper bound can result in an inability to prove the recurrence, but then we should sometimes do a counter intutive think and decrease our guess. This works because we are applying the recurrence to each of the subcases, and so making a big guess initially adds up, and messes up things for later.
Lastly, one mistake to avoid is not proving the exact form of the recurrence and still assuming that you solved it. One needs to be careful and avoid  that.
Some recurrences which look complicated can be reduced to simpler ones by change of variables, say n = lg m or something like that. Look at the exercises for examples.

4.4
This section deals with using Recursion trees to guess the value of a recurrence and then later prove the guess using substitution. We can prove the solution using recurrence trees, but for that we'll have to be rigorous and we would lose the whole purpose of the recursion tree, which is for it to be intutive. 
If there is a recursion relation of the form T(n) = bT(n/a) + theta(f(n))
The cost of T(n) is theta(f(n)) plus theta bT(n/a). So what we do here is sort of open out the recurrence. We make a tree, starting with the root, and each root having b children. Write down theta(f(n)) in the root node. Now in each of its immediate children write down theta(f(n/a)). This represents the contribution of the children towards the running time of the root. Do this for each subtree. Our recursion bottoms out at T(1) which occurs at a depth of 1 + lg_a(n)). There are b^lg_a(n) children, each having cost 1. The total cost of the root, which is what we want to find is the sum of the number in the root and the numbers in all the children. Based on the recurrence, either the root dominates, or the leaves dominate, or the cost is evenly distributed across all levels. In the later case, the run time depends on the sum over levels, whereas in the first two cases, the asymptotic runtime depends heavily only on the root, and the leaves respectively.


4.5
Here we are introduced to the master method for solving recurrences of the form T(n) = aT(n/b) + f(n), a,b >= 1 and f(n) is asymptotically >0. The master method has three cases:
1) f(n) = O(n^(log_b(a) - eplison) for some eplison > 0 gives T(n) = theta(n^log_b(a))
2) f(n) = theta(n^log_b(a)) gives T(n) = theta(f(n) lg n)
3) f(n) = Sigma(n^(log_b(a) + eplison) for some eplison > 0 and if af(n/b) <= cf(n) for some c<1 and sufficiently large n will give T(n) = theta(f(n))

In case 1 and 3, f(n) has to be polynomially smaller and polynomially larger respectively. Otherwise one can't use the master theorem, if say f(n) is lg n times larger than log_b(a). In addition for case three (this part I didn't understand why) af(n/b) <= cf(n), i.e it has to satisfy the regularity condition.

4.6
Here we shall prove the Master theorem. First we prove it for recurrences of the form T(n) = aT(n/b) + f(n) only when n is a multiple of b. And then we generalize it to all n.
Using the Recursion tree for the given recurrence T(n) = theta(n^log_b(a)) + Sum_i[0:log_b(n)-1]. Note that 'a' need not be an integer. Now we bound the Sum in this expression. If f(n) = O(n^log_b(a)-epsilon) for some epsilon>0 then the Sum is O(n^log_b(a)) This can be clearly seen by factoring out terms and simplifying the resulting sum, which gives an decreasing geometric series. This can be bounded using the O notation by O(n^log_b(a)). If f(n) = theta(n^log_b(a)) then the sum shall be a constant sum, with each term being n^log_b(a). Therefore it becomes n^log_b(a) log_b(n), and so we get it to be theta(n^log_b(a) lg n). Now if f(n) = Sigma(n^log_b(a)) then we know that the sum shall be Sigma(f(n)) since f(n) occurs in the Sum and each term of the Sum in non negative. Now if f(n) satisfies the regularity condition i.e. a f(n/b)<= c f(n) for some c<1 and all sufficiently large n, then we can iteratively get a^j f(n/b^j) <= c^j f(n) again for sufficiently large n(i.e upto a certain j only). Now putting this into the Sum, we shall obtain a decreasing geometric series (to an extent) and some constant terms (for those n which are too small to satisfy the given condition,(This set has to be finite)). The decreasing Geometric series (with constant c(<1)) is bounded by O(f(n)), and the constant terms are irrelavent. There in this third case, we have bounded the sum by O(f(n)).
	Now that we have bounded the sum, we proceed to prove the master theorem for exact powers. if f(n) = O(n^(log_b(a) - eplison)) for some epsilon>0 then T(n) will be theta(n^log_b(a)). This proof is relatively straightforward. T(n) = theta(n^log_b(a)) + O(n^log_b(a)) and this is just theta(n^log_b(a)) (as seen in chapter 3). Similarly if f(n) = theta(n^log_b(a)) then T(n) = theta(n^log)b(a) lg n), and this follows from the fact that T(n) = theta(n^log_b(a)) + theta(n^log_b(a) lg n) (as seem in the previous paragraph [bound on the sum]) and this is theta(n^log_b(a) lg n). And finally for f(n) = Sigma(n^(log_b(a) +eplison)) for some eplison > 0 and if a f(n/b) <= c f(n) for some c<1 and all sufficiently large n, then T(n) = theta(f(n)). This is seen from the fact that T(n) = theta(n^log_b(a)) + theta(f(n)) = theta(f(n)) (because f(n) = Sigma(n^log_b(a))).
	To prove this for all natural numbers, i.e when floors and cielings appear in the recurrence we do the following. If T(n) = a T(|- n/b-|) + f(n), then we can see that this is lowerbounded by T(n) = aT(n/b) + f(n) because |-n/b-| is greater than n/b (here |-a-| is the least integer less than or equal to a). Now we have assumed that b>=1. The given recurrence yeilds a sum containing terms at depth j,bounded by n/b^j + b/(b-1). This is obtained by repeatedly using the fact that the least integer function gives a value < n+1 for a given input n. Now taking j as |_log_b(n)_| (greatest integer function) we get the term at depth log_b(n) (actually its greatest integer func) bounded by n/(b^log_b(n)) + b/(b-1)
< n/(b^(log_b(n)-1)) + b/(b-1) = n/(n/b) + b/(b-1). Clearly This is a constant term. Therefore now we see that T(n) = theta(n^log_b(a)) + Sum_j[=:|_log_b(n)_| -1] (a^j f(n_j)). Now this sum can be evaluated as was done previously for exact powers. Here we have proved upper bounds, the proof of lower bounds is similar. And hence we have proved the master theorem. 
