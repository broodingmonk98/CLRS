2.1.1
Initial Array : {31,41,59,26,41,58}
After first iteration of outer for loop:
		{31,41,59,26,41,58}
Second iter :	{31,41,59,26,41,58}
Third iter  :	{26,31,41,59,41,58}
Fourth iter :	{26,31,41,41,59,58}
Fifth iter  :	{26,31,41,41,58,59}
this is the final sorted array.

2.1.2
Refer to program 2.1.2 in programs section.

2.1.3
search(arr,v)
	for i = 0 to n-1
		if(arr[i] == v)
			return i
	return NIL
the loop invariant: in the i'th iteration of the loop, the element is either not in the first i-1 elements, or the eleemnt has been successfully found.
Initialiazation:	This is trivally true for i=0 since, the element isn't in the first -1 elements.
Maintenance    :   Inductively, if the invariant is true for the first i iterations, then in the next iteration, we check whether the i+1 th element is the element we are looking for. If it is, we return that index, and the invariant is true. If it isn't then we move on to the i+2 th iteration, and the element will not be in the first i+1 elements.
Termination 	:  Finally we have gone through all n elements, and so we have either found the element or its not in the given array. Either way the loop invariant would have satisfied the problem stateement.

2.1.4
The problem statement:
Input: two arrays of length n, whhose individual elements are bits.
Output: an n+1 element array of bits, whose bit pattern is the result of the sum of the bit patters of the first two arrays. (Note: here we are using the usual definition of addition of two numbers in the binary system)


2.2.1
Theta(n^3)

2.2.2
Pseudocode for selection sort: Assume arr is the array of n integers.
for i in 0 to n-2
	int small= 0xffffffff, smallIdx=0
	for j in i to n-1
		if small > arr[j]
			small = arr[j]
			smallIdx = j
	arr[j] = arr[i]
	arr[i] = small

This has to run only for the first n-1 elements since, the last element is trivially the smallest in the remaining sub array, namely itself.

The loop invariant: at the end of i th iteration of the outer loop, the first first i elements of the array are in sorted order and are less than any of the elements in the remaining part of the array.
Initialization : for i = 0 this is true, since there is no array, and so an empty array can be defined to be sorted.
Maintainence : if this is true for the i th iteration, then in the i+1st iteration the inner loop takes the smallest element remaining in the last n-i  elements of the array and puts it in the i+1st spot. Now this causes the first i+1 elements to be sorted in non decreasing order, since the element now added is greater than the ones before it, based on the loop invariant.
Termination : At the end of the loop, the whole array is sorted, upto the first n-1 elements. And by then, based on the loop invariant the n th element is also in place, and so the whole array is sorted.

the outer loop runs n-1 times and the inner loop runs n-i times everytime. So the complexity will be the sum of the first n natural numbers  which is Theta(n^2).
This is true for the best case as well as the worst case running times.

2.2.3

The probability of the element we are looking for, being in the first half of the array is the same as that of its being in the second half. To every permutation where it is in the first half, we assign a corresponding permutation in the second half, such that the sum of work done in the two permutations will be n. Proceeding in this way, we see that we have to check n/2 ie half the array on average before finding the element
The probability of the element we are looking for, being in the first half of the array is the same as that of its being in the second half. To every permutation where it is in the first half, we assign a corresponding permutation in the second half, such that the sum of work done in the two permutations will be n. Proceeding in this way, we see that we have to check n/2 ie half the array on average before finding the element.
In the worse case, the element might be the very last one, and then it will have to go through the entire array.

2.2.4

If make give one specific trivial case, and hardcode it and directly give the solution for that one case, then in the best case, ie if the input matches the case for which we have already given the solution, the best case running time will be very low. We can do this with any algorithm.


2.3.1

{3}{41}{52}{26}{38}{57}{9}{49}
{3,41}{26,52}{38,57}{9,49}
{3,26,41,52}{9,38,49,57}
{3,9,26,38,41,49,52,57}

2.3.2

Refer to 2.3.2.c in the programs directory.

2.3.3

For n = 2 this is true.
Assume its true for n.
T(n) = 2T(n/2)+n = n log n
T(2n)=2T(n) + 2n = 2 n log n + 2n = 2n(log n + 1)= 2n log 2n.
Hence proved.

2.3.4

T(n) = T(n-1) + theta(n)

2.3.5

Recursive:
BinarySearch(arr,n,key)//where key is what we are searching for
{
	if(n<1) print(Not found)
	if(arr[n/2] == key)
		print(n/2)
	else if(arr[n/2] > key)
		BinarySearch(arr,n/2,key)
	else
		BinarySearch(arr+n/2, n-n/2,key)
} 
Iterative:
mid = n/2,low=0,high=n-1
while(low<high)
	if(arr[mid] == key)
		print("mid")
	else if(arr[mid]>key)
		high=mid-1;
	else 
		low = mid+1

Runtime: in each version, we are dividing the problem in half, and are doing essentially the same thing. So lets just analyze the recursive case. Here we get the recurrence theta(n) = theta(n/2) + O(1)
let us replace O(1) by a constant c, and then continously expand the resulting recurrence.
theta(n) = theta(n/2) + c = theta(n/4) + c + c = ... = theta(1) + c + c ...(log n times).
Therefore we get theta(n) = theta(1) + log n *c
therefore theta(n) = log n
Running time of binary search on a sorted array is log n.






2.3.6

No we cannot do this since, although we might be able to find the eleement in time log n, since by the loop invariant, the subarray is sorted, we cannot insert it in log n time, since if the element is to be inserted into the middle of the sorted subarray, the second half of the sorted sub array has to be moved one unit to the right in theta(n/2) = theta(n) time and then only can the elemenet be inserted. If we use linked lists to represent the array, then the binary search cant be done, although then insertion can take place in constant time. So However we implement insertion sort, we can't make it work in theta(n log n) time .

2.3.7 * 

This is very similar to merge sort. We recursively subdivide. Then along with the assumption of the sorted array we make the assumption that the two divided subarrays don't have the necessary elements making the sum. So we still have to find the two numbers if they exsist at all. Now sinces the two subarrays are sorted, we begin at the start of one and the end of the other subarray. Now as long as the sum of the two elements is more than x, we continue to move down the second array, and if it becomes equal, we are done. However if it becomes less at any point, we then move to the second element of the first array, but we continue with the current position but one, of the second array. So in this way we essentially make our way through the array, a maximum of 3/2 n times which is theta(n). Then if we still haven't found the eleemnt we merge the two arrays in the usual way, and continue up the recursive tree. 
If we don't find two elements at all, then they don't exsist, and we can conclusively say that its not in the array. which is theta(n). Then if we still haven't found the eleemnt we merge the two arrays in the usual way, and continue up the recursive tree. 
If we don't find two elements at all, then they don't exsist, and we can conclusively say that its not in the array. The work done here is the same as merge sort n log (n). The naive way to do it would be, to find every ordered pair (a,b) and sum them and check which would have taken theta(n^2) time.
