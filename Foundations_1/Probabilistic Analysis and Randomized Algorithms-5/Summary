This chapter introduces algorithms which are not fully deterministic, and we can only instead talk about the worst case/best case scenerio or the more useful expected case/running time.

5.1
We might have an algorithm whose running time varies with each run of the algorithm. For example if we want to hire an assistant, and we interview n people in some order, and at any given time we hire the best person we have seen upto that point, the number of people we end up hiring varies based on the order in which the candidates appear. In the worst case we may need to hire everyone(if they come in increasing order of quality). In the best case we only hire one person. The more useful notion would be that of expected running time. The average case running time is the mean of running time over all inputs.(may be a weighted mean).	On the other hand, randomized algorithms have their running time dependendent on the input and also the output of a random number geenrator, say random(a,b) which returns an integer between a and b, including the end points. We can then take the expectation of the runnimg time, in the case when the algorithm itself makes random choices.

5.2
An indicator random variable (rv), is a rv which takes value 1 if the event occurs and 0 if it doesn't. An Indicator rv of an event A is denoted by I_A. It is useful for relating probability of an event, with expectation (which is useful because expectation is linear). Pr(A) = E(X_A).
Consider flipping n coins. We want to find the expected number of heads, using irv we easily get X=Sum_i(X_i) where X_i is the probability of getting heads on the i th toss. Taking expectation and linearity of expecatation we easily get E(X) = n/2.
To calculate the expected number of times we hire and assistant, let X_i be the irv of hiring the i'th person. Since everyone is uniformly distributed, P(X_i ==1) is 1/i. Let X be the rv denoting the number of people were hire, X = Sum_i (X_i) and using this we get, E(X) = Sum_i (1/i), which gives theta(lg n), which is much better than the worst case bound of theta(n).

5.3
Here we deal with random algorithms, i.e. algorithms that themselves randomize the input so as the ensure good expected performance (no input elicts its worst behaviour (on average)). So the advantage of this is that we don't have to rely on any assumptions on the input distribution, instead controlling the uncertainity ourselves. In the example of hiring a candidate itself, we control the input by permuting the array prior to executing the hire assistant procedure's main body. Now we know for sure that the expectation of cost is theta(lg n). Here we talk about expected hiring cost, whereas previously we talked about average case hiring cost.
On method to permute the input array, is to assign a random number between 1 and n^3 to each element of the array(of size n) and then sort the array using the random numbers as keys. To show that this results in a univform random permutation we proceed as follows: pick a random permutation pi. Let pi(i) represent the value to which pi takes the i'th element. We want to calculate P(pi occuring), which is the same as the intersection of the events, E_i representing i getting assigned the pi(i)'th largest value. The intersection of these events can be expanded as the product of conditional propbabilities of the form P(E_1)*P(E_2|E_1)....P(E_n|E_1^E_2...E_(n-)). The first probability is clearly 1/n. The second probability is 1/(n-1) since each of the remaining elements (after assigning the first element) has an equal chance of being picked. So we get P(pi) = 1/n!. Therefore.
Another method to randomly permute the array is to randomize in place, where we go through i from 1 to n, and at each step we swap the i'th element with a random element choosen between i and n (including the endpoints). To prove that this produces a random permutation of the array, we proceed using a the following loop invariant: prior to the i th iteratiaon of the for loop, each possible (i-1)th permutation of the n elements, are equally likely to be in the subarray A[1...i-1], i.e each has a probability of (n-i+1)!/n! of appearing there.
Initialization: for i = 1, this is vacuously true. (the zero permutation occurs with probability 1). 
Maintainance: Given that it holds for the i'th iteration, for the i+1 th iteration, given that the probability of the first i positions having a particular permutation is (n-i+1)!/n! (by loop invariant), and since we randomly pick the i+1 th element from the remaining part of the array(therefore making it independent of the i permutation), multiplying probabilities of the independent events, we see that the probability of arriving at a particular permutation in the i+1 th iteration is (n-i)!/n! which is what we wanted to show.
Termination: when i=n+1 (termination case), then by the loop invariant we get probability of a particular permutation is 1/n!.
And so the intutive idea has been proved. (see programs section for implementation of these two procedures).

5.4
We use probabilistic analysis here, on some examples to get a better insight into how it works.
How many people should we consider so that the probability that two people have the same birthday is greater than 1/2. Intutively there are nC2 ways of choosing people, and each pair should be distinct and so it should be proportional to root(number of days in a year). To solve it exactly consider the following. Let B_k denote the event of k people having distinct birthdays. Using induction and the assumption that the distribution of bdays are independent, we get B_k = A_k intersection B_(k-1), where A_k is the event that the k th person has a common bday with those coming before him. P(B_k) = P(B_(k-1))*P(A_k|B_(k-1)) and so on. Probabality of P(A_i|B_(i-1)) is (n-i+1)/n. Taking the product we get 1(1-1/n)(1-2/n)...(1-(k+1)/n). Now using 1+x<=e^x, P(B_k) <= e^(-Sum_i(i/n)) = e^(-k(k-1)/2n) <= 1/2. Solving for k gives k>= (1+root(1+8 lg 2 n))/2. So for a earth year, the number of peoples comes out to be >23.
Instead using indicator rv, let X_ij be the irv of i and j having the same bday,Let X be the rv denoting the total number of bdays in common. So X = Sum_i[1,k](Sum_j[i+1,k] X_ij). Taking expectation on both sides, and using its linearity we g et E(X) = nC2 1/n = k(k-1)/2n. We want E(X) = 1, so we get k(k-1)=2n or k > root(n)+1, giving a k value of 28 people. The values in the two different analysis have the same asymptotic rate of increase.
Next we consider the following: we have b bins and we toss balls randomly (uniformly) into the bins. Expected number of balls in a bin, given n balls tossed is n/b(each toss bernolli -> binomial distribution). Number of balls tossed until a given bin contains a ball(on average) is n=b. The more interesting question is what is the number of balls we need to toss, so that every bin contains a ball.
If n_i be the number of tosses between the i-1 th and i th bin getting a ball. E(n_i) (it being a rv) gives b/(b-i+1). E(n) = Sum(E(n_i)) = theta(b lg n).
Now we consider the expected number of streaks in a repeated bernolli experiment. It is of the order theta(lg n), if the experiment is repeated n times, we can prove it using upper and lower bounds, or using indicator rv, let X_ik be the event that at the i th toss, we get a streak of length k. P(X_ik) = 1/2^k. Let X be the rv denoting the number of streaks of length k. So X = Sum_i X_ik. This becomes (n-k+1)/2^k. For k = c lg n we get E(X) = (n- c lg n + 1)/n^c= theta(1/n^(c-1)). So we get expected length of longest streak is theta(n).
Lastly we consider the case of hire assistant, with the constraint that we hire only one person. A strategy which is possible is we interview k of the n candidates, and reject them all, and then pick the one immediately after who has a value greater than the max value in k. If no one has such a value, we end up picking the n th candidate(because once we reject someone, we can't hire them again). To pick the value of k, we want that k which maximizes our chance of hiring the best candidate. Let S be the event of hiring the best candidate. Then P(S) = Sum_i P(S_i) where S_i is the event that the best candidate is hired and he's in position i (for a given k and i>k). P(S_i) = 1/n * k/(i-1). Solving the equation we get that k should be n/e.
